{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATS (Generative Adverserial Tree Search)\n",
    "### A model-based model free appraoch for Deep RL. \n",
    "This tutorial walks through the implementation of Generative Adverserial Tree Search (GATS), \n",
    "an RL method which exploits the experinaces to learn both the model dynamics and the Q function.\n",
    "The model in this tutorial follows the work described in the paper \n",
    "[Surprising Negative Results for Generative Adversarial Tree Search](https://arxiv.org/pdf/1806.05780.pdf), by Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Emma Brunskill, Zachary Lipton, and Animashree Anandkumar\n",
    "\n",
    "Requirement, import the [$OpenAI Gym$](https://gym.openai.com/docs). For domain adaptation, import [$ALE^+Gym$](https://github.com/bclyang/updated-atari-env) by Brandon Yang.\n",
    "\n",
    "\n",
    "## Preliminaries\n",
    "The following code clones and installs the OpenAI gym.\n",
    "`git clone https://github.com/openai/gym ; cd gym ; pip install -e .[all]` \n",
    "Mosly users of AWS EC2 instance need an additional command to be run before intallation:\n",
    "\n",
    "`apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
    "`\n",
    "\n",
    "Full documentation for the gym can be found on [at this website](https://gym.openai.com/).\n",
    "If you want to see reasonable results before the sun sets on your AI career,\n",
    "we suggest running these experiments on a server equipped with GPUs.\n",
    "\n",
    "### Summary of the algorithm\n",
    "\n",
    "For Model free part of GATS, we use DDQN, [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) by Hado van Hasselt. For model based part we develope a U-net artitecture as follows\n",
    "\n",
    "|![](./Fig/gan_generator.png)|\n",
    "|:---------------:|\n",
    "|BDQN Algorithm|\n",
    "\n",
    "We deploy Wassersting metric and spectral normalization technique to train the generative dynamics model, so called GDM. Furhtermore, for reward dynamics, we deploy a simple artitecture. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "\n",
    "from dqn_model import DQN , _RP\n",
    "# from dqn_learn import OptimizerSpec, dqn_learing\n",
    "from utils.gym import get_env, get_env_by_id, get_wrapper_by_name\n",
    "from utils.schedule import LinearSchedule\n",
    "from utils.evaluation import evaluation\n",
    "# from utils.Gradient_penalty import calc_gradient_penalty\n",
    "    \n",
    "import sys\n",
    "import pickle\n",
    "import logging, logging.handlers\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "import gym.spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from utils.replay_buffer import ReplayBuffer, GANReplayBuffer, onehot_action\n",
    "from utils.gym import get_wrapper_by_name\n",
    "from models.models import _netG, _netD, _netPatchD\n",
    "\n",
    "lookahead = 4\n",
    "TARGER_UPDATE_FREQ = 10000\n",
    "skip_f = 4\n",
    "\n",
    "env_name = 'Pong'\n",
    "env_name1 = '%sNoFrameskip-v4' %(env_name)  # Set the desired environment\n",
    "env_name = '%sDDQN-GAN_LA_%d_TargetFQ_%d_skip_%d_RNN' %(env_name1,lookahead,TARGER_UPDATE_FREQ,skip_f)\n",
    "logger = logging.getLogger()\n",
    "file_name = './data/results_%s.log' %(env_name)\n",
    "fh = logging.handlers.RotatingFileHandler(file_name)\n",
    "fh.setLevel(logging.DEBUG)#no matter what level I set here\n",
    "formatter = logging.Formatter('%(asctime)s:%(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "REPLAY_BUFFER_SIZE = 1000000\n",
    "GAN_REPLAY_BUFFER_SIZE = 500000\n",
    "LEARNING_STARTS = 50000\n",
    "LEARNING_STARTS_Q_GAN = 200000\n",
    "REWARD_LEARNING_STARTS = 10000\n",
    "LEARNING_FREQ = 4\n",
    "Reward_learning_freq = 4\n",
    "FRAME_HISTORY_LEN = 4\n",
    "GAN_LEARNING_STARTS = 10000\n",
    "GAN_learning_freq = 4\n",
    "LEARNING_RATE = 0.00025\n",
    "ALPHA = 0.95\n",
    "EPS = 0.01\n",
    "RP_beta1 = 0.5 \n",
    "RP_beta2 = 0.999\n",
    "NUM_ACTIONS = 3\n",
    "NUM_REWARDS = 3\n",
    "NORM_FRAME_VAL = 130.\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "\n",
    "# Run training\n",
    "seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "env = get_env_by_id(env_name1, seed, skip_f)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv' or 'SNConv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "\n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\"])\n",
    "\n",
    "optimizer_spec = OptimizerSpec(\n",
    "    constructor=optim.RMSprop,\n",
    "    kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
    ")\n",
    "\n",
    "\n",
    "exploration_schedule = LinearSchedule(1000000, 0.1)\n",
    "exploration_G = LinearSchedule(50000, 0.01)\n",
    "env=env\n",
    "q_func=DQN\n",
    "optimizer_spec=optimizer_spec\n",
    "exploration=exploration_schedule\n",
    "replay_buffer_size=REPLAY_BUFFER_SIZE\n",
    "gan_replay_buffer_size=GAN_REPLAY_BUFFER_SIZE\n",
    "gan_learning_freq = GAN_learning_freq\n",
    "batch_size=BATCH_SIZE\n",
    "gamma=GAMMA\n",
    "gan_learning_starts=GAN_LEARNING_STARTS\n",
    "learning_starts=LEARNING_STARTS\n",
    "learning_starts_Q_GAN = LEARNING_STARTS_Q_GAN\n",
    "reward_learning_starts=REWARD_LEARNING_STARTS\n",
    "learning_freq=LEARNING_FREQ\n",
    "reward_learning_freq = Reward_learning_freq\n",
    "frame_history_len=FRAME_HISTORY_LEN\n",
    "target_update_freq=TARGER_UPDATE_FREQ\n",
    "reward_bs = 128 \n",
    "bs = 128\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = one * -1\n",
    "num_actions = NUM_ACTIONS\n",
    "norm_frame_val = NORM_FRAME_VAL\n",
    "num_rewards = NUM_REWARDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispimage(num_samples = 1, rollout_len = 3):\n",
    "    obs_batch1, act_batch1, next_obs_batch1 = replay_buffer.GAN_sample(num_samples,rollout_len)\n",
    "    act_batch = onehot_action(act_batch1,num_actions,rollout_len,num_samples)\n",
    "    obs_batch = Variable(norm_frame(torch.from_numpy(obs_batch1).type(dtype)))\n",
    "    next_obs_batch = Variable(norm_frame(torch.from_numpy(next_obs_batch1).type(dtype)))\n",
    "    act_batch = Variable(torch.from_numpy(act_batch).type(dtype))\n",
    "    fig=plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    trajectories = obs_batch\n",
    "    for i in range(rollout_len):\n",
    "        act = Variable(torch.from_numpy(onehot_action(np.expand_dims(act_batch1[:,i],1),num_actions,1,num_samples)).type(dtype))\n",
    "        trajectories = torch.cat((trajectories,norm_frame_Q_GAN(G(trajectories[:,-1*frame_history_len:,:,:],act))), dim = 1)\n",
    "    \n",
    "    for j in range(num_samples):\n",
    "        next_frame = unnorm_frame(next_obs_batch[j]).data.cpu().numpy().astype('uint8')\n",
    "        for i in range(rollout_len):\n",
    "            x = np.expand_dims(next_frame[i], axis=0)\n",
    "            img = np.transpose(np.repeat(x, [3], axis=0), (1, 2, 0))\n",
    "            fig.add_subplot(2, num_samples * rollout_len, j* rollout_len + i+1)\n",
    "            plt.imshow(img)\n",
    "\n",
    "        next_frame = unnorm_frame(trajectories[:,-1*rollout_len:,:,:][j]).data.cpu().numpy().astype('uint8')  \n",
    "        for i in range(rollout_len):\n",
    "            x = np.expand_dims(next_frame[i], axis=0)\n",
    "            img = np.transpose(np.repeat(x, [3], axis=0), (1, 2, 0))\n",
    "            fig.add_subplot(2, num_samples * rollout_len, (num_samples + j) * rollout_len + i + 1)\n",
    "            plt.imshow(img)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def norm_frame(obs):\n",
    "    x = (obs - 127.5)/norm_frame_val\n",
    "    return x\n",
    "\n",
    "def norm_frame_Q(obs):\n",
    "    obs = obs.float()\n",
    "    x = obs/255.\n",
    "    return x\n",
    "\n",
    "def unnorm_frame(obs):\n",
    "    return torch.clamp(obs * norm_frame_val + 127.5,0., 255.).int()\n",
    "\n",
    "def norm_frame_Q_GAN(obs):\n",
    "    return torch.clamp(obs,-1*127.5/norm_frame_val, 127.5/norm_frame_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space.n)\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert type(env.observation_space) == gym.spaces.Box\n",
    "assert type(env.action_space)      == gym.spaces.Discrete\n",
    "\n",
    "img_h, img_w, img_c = env.observation_space.shape\n",
    "input_arg = frame_history_len * img_c\n",
    "\n",
    "# Set the base tree\n",
    "leaves_size = num_actions**lookahead\n",
    "def base_generator ():\n",
    "    tree_base = np.zeros((leaves_size,lookahead)).astype('uint8')\n",
    "    for i in range(leaves_size):\n",
    "        n = i\n",
    "        j = 0\n",
    "        while n:\n",
    "            n, r = divmod(n, num_actions)\n",
    "            tree_base[i,lookahead-1-j] = r\n",
    "            j = j + 1\n",
    "    tree_base_onehot = torch.from_numpy(onehot_action(tree_base,num_actions,lookahead,leaves_size)).type(dtype)\n",
    "    return tree_base, tree_base_onehot\n",
    "\n",
    "tree_base, tree_base_onehot = base_generator()\n",
    "\n",
    "# MCTS planner\n",
    "# base_3 = torch.from_numpy(onehot_action(np.arange(3).reshape((3,1)),num_actions,1,3)).type(dtype)\n",
    "\n",
    "# def state_generator(G,state,depth):\n",
    "#     if depth == lookahead-1:\n",
    "#         G(state,base_3)\n",
    "#     else:\n",
    "#         state_generator(state,depth+1):\n",
    "        \n",
    "    \n",
    "def MCTS_planning(G, Q, RP, state, t):\n",
    "    sample1 = random.random()\n",
    "    sample2 = random.random()\n",
    "    eps_threshold = exploration.value(t)\n",
    "\n",
    "    state = Variable(state.repeat(leaves_size,1,1,1))\n",
    "    trajectories = state\n",
    "    for i in range(lookahead):\n",
    "        act = Variable(torch.from_numpy(onehot_action(np.expand_dims(tree_base[:,i],1),num_actions,1,leaves_size)).type(dtype))\n",
    "        trajectories = torch.cat((trajectories,norm_frame_Q_GAN(G(trajectories[:,-1*frame_history_len:,:,:],act))), dim = 1)\n",
    "#     trajectories = torch.cat((state,norm_frame_Q_GAN(G(state,var_tree_base_onehot))), dim = 1)\n",
    "    leaves_Q = Q(norm_frame_Q(unnorm_frame(trajectories[:,-frame_history_len:,:,:])))\n",
    "    leaves_Q_max, leaves_act_max = leaves_Q.data.max(1)\n",
    "    leaves_Q_max = gamma **(lookahead) * leaves_Q_max\n",
    "    leaves_act_max = np.expand_dims(leaves_act_max.cpu().numpy(), axis=1)\n",
    "    if sample2 < eps_threshold:\n",
    "        leaves_act_max = np.random.randint(0, num_actions, leaves_act_max.shape)\n",
    "    reward_actions = np.concatenate((tree_base, leaves_act_max), axis=1)\n",
    "    var_reward_actions_onehot = Variable(torch.from_numpy(onehot_action(reward_actions, num_actions, lookahead + 1, leaves_size)).type(dtype))\n",
    "    # check RP indexing\n",
    "    predicted_cum_rew = RP(norm_frame_Q_GAN(trajectories.detach()),var_reward_actions_onehot)\n",
    "    predicted_cum_return = torch.zeros(leaves_size).type(dtype)\n",
    "    for i in range(lookahead):\n",
    "        predicted_cum_return = gamma * predicted_cum_return + \\\n",
    "            (predicted_cum_rew.data[:,((lookahead-i-1)*num_rewards):((lookahead-i)*num_rewards)].max(1)[1]-1).type(dtype)\n",
    "    GATS_action = (leaves_Q_max + predicted_cum_return).cpu().numpy()\n",
    "    max_idx = GATS_action.argmax(0)\n",
    "    return_action = int(tree_base[max_idx,0])\n",
    "    # update gan replay buffer\n",
    "    if sample1 < eps_threshold:\n",
    "        max_idx = random.randrange(leaves_size)\n",
    "    obs = unnorm_frame(trajectories[max_idx, lookahead:lookahead + frame_history_len, :, :]).data.cpu().numpy().astype('uint8')\n",
    "    act_batch = np.squeeze(leaves_act_max[max_idx])\n",
    "    rew_batch = (predicted_cum_rew[max_idx,-num_rewards:].max(0)[1] - 1).data.cpu().numpy()\n",
    "    gan_replay_buffer.add_batch(obs, act_batch, rew_batch)\n",
    "    return return_action\n",
    "\n",
    "# Construct an epilson greedy policy with given exploration schedule\n",
    "def select_epilson_greedy_action(Q,G,RP, state, t):\n",
    "    sample = random.random()\n",
    "    eps_threshold = exploration.value(t)\n",
    "    if sample > eps_threshold:\n",
    "        state = norm_frame(torch.from_numpy(state).type(dtype).unsqueeze(0))\n",
    "        if t < learning_starts_Q_GAN:\n",
    "            return Q(norm_frame_Q(unnorm_frame(Variable(state, volatile=True)))).detach().data.max(1)[1].cpu()\n",
    "        else:\n",
    "#           Use volatile = True if variable is only used in inference mode, i.e. donâ€™t save the history\n",
    "            return MCTS_planning(G, Q, RP, state, t)\n",
    "    else:\n",
    "        return torch.IntTensor([[random.randrange(num_actions)]])\n",
    "\n",
    "# Initialize target q function and q function\n",
    "Q = q_func(input_arg, num_actions).type(dtype)\n",
    "# Q_GAN = q_func(input_arg, num_actions).type(dtype)\n",
    "# Q_GAN.load_state_dict(Q.state_dict())\n",
    "target_Q = q_func(input_arg, num_actions).type(dtype)\n",
    "\n",
    "# Construct Q network optimizer function\n",
    "optimizer = optimizer_spec.constructor(Q.parameters(), **optimizer_spec.kwargs)\n",
    "# optimizer_Q_GAN = torch.optim.Adam(Q_GAN.parameters(),lr = 1e-4,weight_decay = 0., betas = (RP_beta1, RP_beta2)) \n",
    "\n",
    "\n",
    "# losses\n",
    "softmax_cross_entropy_loss = nn.CrossEntropyLoss().cuda()\n",
    "L1_loss = nn.L1Loss()\n",
    "L2_loss = nn.MSELoss()\n",
    "lossQ_GAN = torch.nn.MSELoss().cuda()\n",
    "# Reward predictor\n",
    "\n",
    "RP = _RP(num_actions,num_rewards,lookahead,frame_history_len).type(dtype)\n",
    "RP.apply(weights_init)\n",
    "trainerR = torch.optim.Adam(RP.parameters(),lr = 2e-4,weight_decay = 0.0001, betas = (RP_beta1, RP_beta2)) \n",
    "\n",
    "\n",
    "# Construct the replay buffer\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "gan_replay_buffer = GANReplayBuffer(gan_replay_buffer_size, frame_history_len)\n",
    "\n",
    "\n",
    "# GDM\n",
    "critic_iters = 1\n",
    "actor_iters = 1\n",
    "G_lr = 1e-4\n",
    "D_lr = 1e-5\n",
    "lambda_l1_gan = 20.\n",
    "lambda_l2_gan = 80. #5, 0.0002\n",
    "gan_bs = 128\n",
    "gan_warmup = 5000\n",
    "\n",
    "G = _netG(in_channels=frame_history_len, num_actions = num_actions, lookahead = 1, ngf=24).cuda()\n",
    "SND = _netD(in_channels=frame_history_len, lookahead = lookahead , num_actions=num_actions, ngf=24).cuda()\n",
    "G.apply(weights_init)\n",
    "SND.apply(weights_init)\n",
    "\n",
    "trainerG = optim.Adam(G.parameters(), weight_decay = 0.001,lr=G_lr, betas=(0.5, 0.999))\n",
    "trainerSND = optim.SGD(SND.parameters(), weight_decay = 0.1, lr = D_lr, momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_param_updates = 0\n",
    "LOG_EVERY_N_STEPS = 10000\n",
    "\n",
    "tot_clipped_reward = []\n",
    "tot_reward = []\n",
    "frame_count_record = []\n",
    "tot_clipped_reward_epi = []\n",
    "tot_reward_epi = []\n",
    "frame_count_record_epi = []\n",
    "\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0.\n",
    "cum_clipped_reward = 0\n",
    "cum_reward = 0\n",
    "cum_clipped_reward_epi = 0\n",
    "cum_reward_epi = 0\n",
    "epi = 0\n",
    "t = 0\n",
    "last_obs = env.reset()\n",
    "tot_D = []\n",
    "tot_G = []\n",
    "tot_l2 = []\n",
    "tot_l1 = []\n",
    "tot_Q_GAN = []\n",
    "tot_rew_err = []\n",
    "tot_rew_err_nonzero = []\n",
    "\n",
    "\n",
    "gen_steps = 0\n",
    "tot_steps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import time\n",
    "def renderimage(next_frame):\n",
    "    if render_image:\n",
    "        next_frame = np.repeat(next_frame, [3], axis=2)\n",
    "        plt.imshow(next_frame);\n",
    "        plt.show()\n",
    "#         display.clear_output(wait=True)\n",
    "#         time.sleep(.1)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(G))\n",
    "print(count_parameters(SND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_image = False\n",
    "\n",
    "while t < 40000001:\n",
    "    t = t + 1 \n",
    "    if t == 10000:\n",
    "        reward_learning_freq = 8\n",
    "        gan_learning_freq = 8\n",
    "    if t == 50000:\n",
    "        reward_learning_freq = 16\n",
    "        gan_learning_freq = 16\n",
    "    if t == 100000:\n",
    "        reward_learning_freq = 24\n",
    "        gan_learning_freq = 24\n",
    "    last_idx = replay_buffer.store_frame(last_obs)\n",
    "    recent_observations = replay_buffer.encode_recent_observation()\n",
    "    if t> learning_starts:\n",
    "        action = select_epilson_greedy_action(Q, G , RP , recent_observations, t)\n",
    "    else:\n",
    "        action = random.randrange(num_actions)\n",
    "    apply_action = action\n",
    "    if int(action != 0):\n",
    "        apply_action = action + 1\n",
    "        \n",
    "    obs, reward, done, done_epi, _ = env.step(apply_action)\n",
    "    renderimage(obs)\n",
    "    # clip rewards between -1 and 1\n",
    "    cum_reward += reward\n",
    "    cum_reward_epi += reward\n",
    "    reward = max(-1.0, min(reward, 1.0))\n",
    "    cum_clipped_reward += reward\n",
    "    cum_clipped_reward_epi += reward    \n",
    "    # Store other info in replay memory\n",
    "    replay_buffer.store_effect(last_idx, action, reward, done)\n",
    "    # Resets the environment when reaching an episode boundary.\n",
    "    \n",
    "    if t % 50000. == 0. :\n",
    "        logging.error('env:%s,epis[%d],durat[%d],fnum=%d, cum_cl_rew = %d, cum_rew = %d,tot_cl = %d , tot = %d'\\\n",
    "          %(env_name, epi,frame_count_record[-1]-frame_count_record[-2],t,tot_clipped_reward[-1],tot_reward[-1],moving_average_clipped,moving_average))\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        epi = epi + 1\n",
    "        tot_clipped_reward = np.append(tot_clipped_reward, cum_clipped_reward)\n",
    "        tot_reward = np.append(tot_reward, cum_reward)\n",
    "        if done_epi:\n",
    "            tot_clipped_reward_epi = np.append(tot_clipped_reward_epi, cum_clipped_reward_epi)\n",
    "            tot_reward_epi = np.append(tot_reward_epi, cum_reward_epi)\n",
    "            frame_count_record_epi = np.append(frame_count_record_epi,t)\n",
    "            cum_clipped_reward_epi = 0\n",
    "            cum_reward_epi = 0\n",
    "        cum_clipped_reward = 0\n",
    "        cum_reward = 0\n",
    "        frame_count_record = np.append(frame_count_record,t)\n",
    "        moving_average_clipped = np.mean(tot_clipped_reward[-1*min(100,epi):])\n",
    "        moving_average = np.mean(tot_reward[-1*min(100,epi):])\n",
    "    if t % 100000 == 0:\n",
    "        fnam = './data/clippted_rew_%s' %(env_name)\n",
    "        np.save(fnam,tot_clipped_reward)\n",
    "        fnam = './data/tot_rew_%s' %(env_name)\n",
    "        np.save(fnam,tot_reward)\n",
    "        fnam = './data/frame_count_%s' %(env_name)\n",
    "        np.save(fnam,frame_count_record)\n",
    "        fnam = './data/rew_err_%s' %(env_name)\n",
    "        fnam = './data/clippted_rew_epi_%s' %(env_name)\n",
    "        np.save(fnam,tot_clipped_reward_epi)\n",
    "        fnam = './data/tot_rew_epi_%s' %(env_name)\n",
    "        np.save(fnam,tot_reward_epi)\n",
    "        fnam = './data/frame_count_epi_%s' %(env_name)\n",
    "        np.save(fnam,frame_count_record_epi)\n",
    "        fnam = './data/rew_err_%s' %(env_name)\n",
    "        np.save(fnam,tot_rew_err)\n",
    "        fnam = './data/rew_err_nonzero_%s' %(env_name)\n",
    "        np.save(fnam,tot_rew_err_nonzero)\n",
    "        fnam = './data/tot_D_%s' %(env_name)\n",
    "        np.save(fnam,tot_D)\n",
    "        fnam = './data/tot_G_%s' %(env_name)\n",
    "        np.save(fnam,tot_G) \n",
    "        fnam = './data/tot_l2_%s' %(env_name)\n",
    "        np.save(fnam,tot_l2)\n",
    "        fnam = './data/tot_l1_%s' %(env_name)\n",
    "        np.save(fnam,tot_l1)\n",
    "#         fnam = './data/tot_Q_GAN_%s' %(env_name)\n",
    "#         np.save(fnam,tot_Q_GAN)\n",
    "\n",
    "    if t % 100000 == 0:\n",
    "        fdqn = './data/target_%s_%d' % (env_name,int(epi / 10))\n",
    "        torch.save(Q.state_dict(), fdqn)\n",
    "            \n",
    "    last_obs = obs\n",
    "    \n",
    "    # Learning the RP:\n",
    "    if (t > reward_learning_starts and\n",
    "            t % reward_learning_freq == 0 and\n",
    "            replay_buffer.can_sample(bs)):\n",
    "        obs, act, rew = replay_buffer.reward_sample(bs,lookahead)\n",
    "        reward_obs, reward_act, reward_rew = replay_buffer.nonzero_reward_sample(reward_bs,lookahead)\n",
    "        obs_batch = Variable(norm_frame(torch.from_numpy(np.concatenate((obs,reward_obs),axis=0)).type(dtype)))\n",
    "        act_ = np.concatenate((act,reward_act),axis=0)\n",
    "        act_batch = onehot_action(act_,num_actions,lookahead + 1,bs+reward_bs)\n",
    "        act_batch = Variable(torch.from_numpy(act_batch).type(dtype))\n",
    "        rew_batch = Variable(torch.from_numpy(np.concatenate((rew,reward_rew),axis=0)).long().cuda())\n",
    "        reward_labels = rew_batch + 1\n",
    "        trainerR.zero_grad()\n",
    "        loss = 0.\n",
    "\n",
    "        trajectories = obs_batch\n",
    "        for ijk in range(lookahead):\n",
    "            act = Variable(torch.from_numpy(onehot_action(np.expand_dims(act_[:,ijk],1),num_actions,1,reward_bs+bs)).type(dtype))\n",
    "            trajectories = torch.cat((trajectories,norm_frame_Q_GAN(G(trajectories[:,-1*frame_history_len:,:,:],act))), dim = 1)\n",
    "\n",
    "        predicted_cum_rew = RP(norm_frame_Q_GAN(trajectories.detach()),act_batch)\n",
    "        for ind in range(lookahead + 1):\n",
    "            outputs = predicted_cum_rew[:,num_rewards * ind: num_rewards * (ind + 1)]\n",
    "            loss = loss + softmax_cross_entropy_loss(outputs, reward_labels[:,ind,0])\n",
    "\n",
    "        loss.backward()\n",
    "        trainerR.step()\n",
    "\n",
    "        if t % 100000 == 0:\n",
    "            reg_rew, non_ze = evaluation(RP,G,replay_buffer,norm_frame,norm_frame_Q_GAN,lookahead,num_actions,frame_history_len,num_rewards)\n",
    "            logging.error('Accuracy of RP model on rewards:%d and on non_zero %d' %(reg_rew, non_ze))\n",
    "            tot_rew_err = np.append(tot_rew_err, reg_rew)\n",
    "            tot_rew_err_nonzero = np.append(tot_rew_err_nonzero, non_ze)\n",
    "\n",
    "\n",
    "    # Learning the GDM:\n",
    "    if (t > gan_learning_starts and\n",
    "            t % gan_learning_freq == 0 and\n",
    "            replay_buffer.can_sample(gan_bs)):\n",
    "        for ii in range(critic_iters):\n",
    "            obs_batch, act_batch_, next_obs_batch = replay_buffer.GAN_sample(gan_bs,lookahead)\n",
    "            act_batch = onehot_action(act_batch_,num_actions,lookahead,gan_bs)\n",
    "            obs_batch = Variable(norm_frame(torch.from_numpy(obs_batch).type(dtype)))\n",
    "            next_obs_batch = Variable(norm_frame(torch.from_numpy(next_obs_batch).type(dtype)))\n",
    "            act_batch = Variable(torch.from_numpy(act_batch).type(dtype))\n",
    "\n",
    "            SND.zero_grad()\n",
    "            cat_real = torch.cat((obs_batch,next_obs_batch), dim = 1)\n",
    "\n",
    "            cat_fake = obs_batch\n",
    "            trajectories = obs_batch\n",
    "            for ijk in range(lookahead):\n",
    "                act = Variable(torch.from_numpy(onehot_action(np.expand_dims(act_batch_[:,ijk],1),num_actions,1,gan_bs)).type(dtype))\n",
    "                if gen_steps > gan_warmup:\n",
    "                    sample = random.random()\n",
    "                    eps_threshold = exploration_G.value(gen_steps-gan_warmup)\n",
    "                else:\n",
    "                    sample = 1\n",
    "                    eps_threshold = 0\n",
    "                fake = norm_frame_Q_GAN(G(trajectories[:,-1*frame_history_len:,:,:].detach(),act).detach())\n",
    "                cat_fake = torch.cat((cat_fake,fake), dim = 1)\n",
    "                if sample > eps_threshold:\n",
    "                    trajectories = torch.cat((trajectories, fake), dim = 1)\n",
    "                else:\n",
    "                    trajectories = torch.cat((trajectories,next_obs_batch[:,ijk,:,:].unsqueeze(1)), dim = 1) \n",
    "\n",
    "            D_real = SND(cat_real,act_batch).mean()\n",
    "            D_real_neg = -1 * D_real\n",
    "            D_real_neg.backward()\n",
    "\n",
    "            D_fake = SND(cat_fake,act_batch).mean()\n",
    "            D_fake.backward()\n",
    "\n",
    "            D_cost = D_fake - D_real \n",
    "            Wasserstein_D = D_real - D_fake\n",
    "            trainerSND.step()\n",
    "            tot_steps += 1\n",
    "            # Q_GAN\n",
    "#             if (t > learning_starts_Q_GAN):\n",
    "#                 optimizer_Q_GAN.zero_grad()\n",
    "#                 Q_GAN_loss = lossQ_GAN(Q_GAN(norm_frame_Q_GAN(cat_fake[:,-1*(frame_history_len):,:,:].detach())),\\\n",
    "#                                               Q(cat_real[:,-1*(frame_history_len):,:,:]).detach()).mean()\n",
    "\n",
    "#                 Q_GAN_loss.backward()\n",
    "#                 optimizer_Q_GAN.step()\n",
    "        tot_D = np.append(tot_D, Wasserstein_D.data.cpu().numpy())\n",
    "        \n",
    "        for ii in range(actor_iters):\n",
    "            obs_batch, act_batch_, next_obs_batch = replay_buffer.GAN_sample(gan_bs,lookahead)\n",
    "            act_batch = onehot_action(act_batch_,num_actions,lookahead,gan_bs)\n",
    "            obs_batch = Variable(norm_frame(torch.from_numpy(obs_batch).type(dtype)))\n",
    "            next_obs_batch = Variable(norm_frame(torch.from_numpy(next_obs_batch).type(dtype)))\n",
    "            act_batch = Variable(torch.from_numpy(act_batch).type(dtype))\n",
    "\n",
    "            G.zero_grad()\n",
    "\n",
    "            cat_fake = obs_batch\n",
    "            trajectories = obs_batch\n",
    "            for ijk in range(lookahead):\n",
    "                act = Variable(torch.from_numpy(onehot_action(np.expand_dims(act_batch_[:,ijk],1),num_actions,1,gan_bs)).type(dtype))\n",
    "                if gen_steps > gan_warmup:\n",
    "                    sample = random.random()\n",
    "                    eps_threshold = exploration_G.value(gen_steps-gan_warmup)\n",
    "                else:\n",
    "                    sample = 1\n",
    "                    eps_threshold = 0\n",
    "                fake = norm_frame_Q_GAN(G(trajectories[:,-1*frame_history_len:,:,:].detach(),act))\n",
    "                cat_fake = torch.cat((cat_fake, fake), dim = 1)\n",
    "                if sample > eps_threshold:\n",
    "                    trajectories = torch.cat((trajectories, fake), dim = 1)\n",
    "                else:\n",
    "                    trajectories = torch.cat((trajectories, next_obs_batch[:,ijk,:,:].unsqueeze(1)), dim = 1) \n",
    "\n",
    "            cat_real = torch.cat((obs_batch,next_obs_batch), dim = 1)\n",
    "            G_fake = SND(cat_fake,act_batch).mean()\n",
    "            G_cost = G_fake\n",
    "            l1_loss = L1_loss(cat_fake[:,-1*lookahead:,:,:],next_obs_batch).mean()\n",
    "            l2_loss = L2_loss(cat_fake[:,-1*lookahead:,:,:], next_obs_batch).mean()\n",
    "            G_fake = l2_loss * lambda_l2_gan  + l1_loss * lambda_l1_gan - G_fake\n",
    "            G_fake.backward()\n",
    "            trainerG.step()\n",
    "            gen_steps += 1\n",
    "            tot_steps += 1\n",
    "            # Q_GAN\n",
    "#             if (t > learning_starts_Q_GAN):\n",
    "#                 optimizer_Q_GAN.zero_grad()\n",
    "#                 Q_GAN_loss = lossQ_GAN(Q_GAN(norm_frame_Q_GAN(cat_fake[:,-1*(frame_history_len):,:,:].detach())),\\\n",
    "#                                               Q(cat_real[:,-1*(frame_history_len):,:,:]).detach()).mean()\n",
    "#                 Q_GAN_loss.backward()\n",
    "#                 optimizer_Q_GAN.step()\n",
    "#                 loss_Q = lossQ_GAN(Q(norm_frame_Q_GAN(cat_fake[:,-1*(frame_history_len):,:,:].detach())),\\\n",
    "#                                                   Q(cat_real[:,-1*(frame_history_len):,:,:]).detach()).mean()\n",
    "#                 if (loss_Q<Q_GAN_loss).data.cpu().numpy():\n",
    "#                     optimizer_Q_GAN = torch.optim.Adam(Q_GAN.parameters(),lr = 1e-4,weight_decay = 0., betas = (RP_beta1, RP_beta2)) \n",
    "#                     Q_GAN.load_state_dict(Q.state_dict())\n",
    "       \n",
    "        tot_G = np.append(tot_G, G_cost.data.cpu().numpy())\n",
    "        tot_l1 = np.append(tot_l1, l1_loss.data.cpu().numpy())\n",
    "        tot_l2 = np.append(tot_l2, l2_loss.data.cpu().numpy())\n",
    "#         if (t > learning_starts_Q_GAN):\n",
    "#             tot_Q_GAN = np.append(tot_Q_GAN, Q_GAN_loss.data.cpu().numpy())\n",
    "#         else:\n",
    "#             tot_Q_GAN = np.append(tot_Q_GAN, 0 )\n",
    "        if gen_steps % 500 == 0:\n",
    "            print('[%d] D: %.4f G: %.4f L2 = %.4f Gen_steps = %d Tot_steps = %d'\n",
    "                  % (t,Wasserstein_D.data.cpu().numpy(), G_cost.data.cpu().numpy(), l2_loss.data.cpu().numpy(), gen_steps, tot_steps))\n",
    "            G.eval()\n",
    "            dispimage(num_samples = 1, rollout_len = lookahead)\n",
    "            G.train()\n",
    "            \n",
    "            fonts = 10\n",
    "            tim = np.arange(len(tot_G))\n",
    "            belplt = plt.plot(tim,tot_G,\"b\", label = \"tot_G\")\n",
    "            belplt = plt.plot(tim,tot_D,\"c\", label = \"tot_D\")\n",
    "            belplt = plt.plot(tim,tot_l2,\"r\", label = \"tot_l2\")\n",
    "            belplt = plt.plot(tim,tot_l1,\"g\", label = \"tot_l1\")\n",
    "#             belplt = plt.plot(tim,tot_Q_GAN,\"m\", label = \"tot_Q_GAN\")\n",
    "            plt.legend(fontsize=fonts)\n",
    "            plt.ylabel(\"loss\",fontsize=fonts, family = 'serif')\n",
    "            plt.title(\"%s - D_lr: %f, G_lr: %f, l_l2: %f, bs %d\" %(env_name, D_lr, G_lr, lambda_l2_gan, gan_bs),fontsize=fonts, family = 'serif')\n",
    "            plt.show()\n",
    "\n",
    "    # Learning Q\n",
    "    if (t > learning_starts and t % learning_freq == 0 and replay_buffer.can_sample(batch_size)):\n",
    "        obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size,lookahead)\n",
    "\n",
    "        obs_batch = Variable(norm_frame(torch.from_numpy(obs_batch).type(dtype)))\n",
    "        act_batch = Variable(torch.from_numpy(act_batch).long())\n",
    "        rew_batch = Variable(torch.from_numpy(rew_batch).type(dtype))\n",
    "        next_obs_batch = Variable(norm_frame(torch.from_numpy(next_obs_batch).type(dtype)))\n",
    "        not_done_mask = Variable(torch.from_numpy(1-done_mask).type(dtype))\n",
    "\n",
    "        total_batch_size = batch_size\n",
    "        if t > learning_starts_Q_GAN and gan_replay_buffer.can_sample(batch_size):\n",
    "            gan_obs_batch, gan_act_batch, gan_rew_batch = gan_replay_buffer.sample_batch(batch_size)\n",
    "            onehot_act_batch = onehot_action(np.expand_dims(gan_act_batch, axis=1),num_actions,1,batch_size)\n",
    "            gan_obs_batch = Variable(norm_frame(torch.from_numpy(gan_obs_batch).type(dtype)))\n",
    "            gan_act_batch = Variable(torch.from_numpy(gan_act_batch).long())\n",
    "            gan_rew_batch = Variable(torch.from_numpy(gan_rew_batch).type(dtype))\n",
    "            onehot_act_batch = Variable(torch.from_numpy(onehot_act_batch).type(dtype))\n",
    "            gan_next_frame = norm_frame_Q_GAN(G(gan_obs_batch.detach(),onehot_act_batch))\n",
    "            gan_next_obs_batch = torch.cat((gan_obs_batch[:,-frame_history_len+1:,:,:], gan_next_frame), dim=1)\n",
    "            gan_not_done_mask = torch.ones_like(not_done_mask)\n",
    "\n",
    "            obs_batch = torch.cat((obs_batch, gan_obs_batch), dim=0)\n",
    "            act_batch = torch.cat((act_batch, gan_act_batch), dim=0)\n",
    "            rew_batch = torch.cat((rew_batch, gan_rew_batch), dim=0)\n",
    "            next_obs_batch = torch.cat((next_obs_batch, gan_next_obs_batch), dim=0)\n",
    "            not_done_mask = torch.cat((not_done_mask, gan_not_done_mask), dim=0)\n",
    "\n",
    "            total_batch_size = batch_size * 2\n",
    "\n",
    "        if USE_CUDA:\n",
    "            act_batch = act_batch.cuda()\n",
    "\n",
    "        current_Q_values = Q(norm_frame_Q(unnorm_frame(obs_batch))).gather(1, act_batch.unsqueeze(1)).squeeze(1)\n",
    "#         ddqn_action = Q(norm_frame_Q(unnorm_frame(next_obs_batch))).max(1)[1]\n",
    "        next_max_q = target_Q(norm_frame_Q(unnorm_frame(next_obs_batch))).detach()[torch.arange(total_batch_size).type(torch.cuda.LongTensor).unsqueeze(1),\\\n",
    "                          Q(norm_frame_Q(unnorm_frame(next_obs_batch))).max(1)[1].data.unsqueeze(1)].squeeze(1)\n",
    "        next_Q_values = not_done_mask * next_max_q\n",
    "        target_Q_values = rew_batch + (gamma * next_Q_values)\n",
    "        bellman_error = target_Q_values - current_Q_values\n",
    "        clipped_bellman_error = bellman_error.clamp(-1, 1)\n",
    "        d_error = clipped_bellman_error * -1.0\n",
    "        optimizer.zero_grad()\n",
    "        current_Q_values.backward(d_error.data)\n",
    "\n",
    "        optimizer.step()\n",
    "        num_param_updates += 1\n",
    "\n",
    "        if num_param_updates % target_update_freq == 0:\n",
    "            target_Q.load_state_dict(Q.state_dict())\n",
    "            \n",
    "            \n",
    "    if t % 500000 == 0:\n",
    "        fdqn = './data/GAN_%s_%d' % (env_name,t)\n",
    "        torch.save(G.state_dict(), fdqn)\n",
    "        fdqn = './data/SND_%s_%d' % (env_name,t)\n",
    "        torch.save(SND.state_dict(), fdqn)\n",
    "        fdqn = './data/Reward_%s_%d' % (env_name,t)\n",
    "        torch.save(RP.state_dict(), fdqn)\n",
    "#         fdqn = './data/Q_GAN_%s_%d' % (env_name,t)\n",
    "#         torch.save(Q_GAN.state_dict(), fdqn)\n",
    "        fdqn = './data/Q_%s_%d' % (env_name,t)\n",
    "        torch.save(Q.state_dict(), fdqn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
